---
title: "emileygz_FinalHomeworkCode_04"
author: "Emiley Garcia-Zych"
date: "`r Sys.Date()`"
output: html_document
---

# Homework 4: What's Your Malfunction?

## Emiley Garcia-Zych

### [1] Write a simple R function, z.prop.test(), that can perform one- or two- sample Z-tests for proportion data, using the following guidelines:

```{r}
Z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, alternative = "two.sided", conf.level = 0.95) { # defines a new fucntion "Z.prop.test()" that takes the arguments "p1" and "p2", sample sizes "n1" and "n2", null hypothesis proportion "p0", the type of test, and confidence level of 95%. If p2 and n2 are not provided, the test will be able to run a one-sample test. 
  
  one_sample_test <- is.null(p2) || is.null(n2) # Checks if either p2 or n2 is NULL to determine one-sample or two-sample test
  
  # Calculate the estimated proportion in a two-sample case
  if (one_sample_test) {
    p1_hat <- p1
  } 
  else {
    p1_hat <- p1
    p2_hat <- p2
  }
  
  # Check rules of thumb
  if (n1 * p1_hat > 5 && n1 * (1 - p1_hat) > 5 && (one_sample_test || (n2 * p2_hat > 5 && n2 * (1 - p2_hat) > 5))) { ##makes sure both samples are large enough to run the tests. 
    standard_error <- sqrt(p1_hat * (1 - p1_hat) / n1) # Calculate the standard error
    
    if (!one_sample_test) { #checks to see if it is a two-sample test. If it is a one-sample test, it continues to the else...
      standard_error2 <- sqrt(p2_hat * (1 - p2_hat) / n2)
      pooled_standard_error <- sqrt((standard_error^2 / n1) + (standard_error2^2 / n2))
      Z_score <- (p1_hat - p2_hat) / pooled_standard_error ##calculates Z statistic 
    } 
    else {
      Z_score <- (p1_hat - p0) / standard_error
    }
    
    if (alternative == "greater") {
      P <- 1 - pnorm(Z)
    } else if (alternative == "less") {
      P <- pnorm(Z)
    } else {
      P <- 2 * (1 - pnorm(abs(Z)))
    }
    
    # Calculate the confidence interval
    alpha <- 1 - conf.level
    if (one_sample_test) {
      margin_of_error <- qnorm(1 - alpha / 2) * standard_error
      CI <- c(p1_hat - margin_of_error, p1_hat + margin_of_error)
    } else {
      margin_of_error <- qnorm(1 - alpha / 2) * pooled_standard_error
      CI <- c(p1_hat - p2_hat - margin_of_error, p1_hat - p2_hat + margin_of_error)
    } 
    
    result <- list(Z = Z, P = P, CI = CI)
    return(result)
  } else { ##returns a warning and lets the user know there was some issue with the data. 
    warning("The rules of thumb (n*p > 5 and n*(1-p) > 5) are violated. The results may not be valid.")
    return(NULL)
  }
}

```

### [2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (`MaxLongevity_m`) measured in months from species' brain size (`Brain_Size_Species_Mean`) measured in grams. Do the following for both `longevity~brain size` and `log(longevity)~log(brain size)`:

```{r}
library ('curl')
k_and_c <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
d <- read.csv(k_and_c, header = TRUE, sep = ",", stringsAsFactors = FALSE)

d <- na.omit(d) ## omit incomplete data
```

#### [a] Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function `geom_text()`).

##### *For longevity \~ brain size*

```{r}
library(ggplot2)

longevity <- d$MaxLongevity_m ##establish longevity vector
brain_size <- d$Brain_Size_Species_Mean ##establish brain size vector

# Fit longevity~brain size model
mod1 <- lm(longevity ~ brain_size, data = d) 

# Extract coefficients from the model
intercept <- round(coef(mod1)[1], 2)
slope <- round(coef(mod1)[2], 2)

# Create the equation text
eq_text <- paste("Model 1: y =", slope, "x", "+", intercept)

plot1 <- ggplot(d, aes((brain_size), (longevity), label = rownames(d))) + geom_point() + 
geom_smooth(method = "lm", formula = y ~ x, color = "#F17F29") + ##draw line
labs(title = "Scatterplot with Fitted Line", x = "Brain Size (Species Mean)", y = "Max Longevity (Months)") + ##title
geom_text(aes(x = 100, y = 750, label = eq_text), size = 4, color = "#F17F29") ##write equation

plot1 ##print
```

##### *For log(longevity) \~ log(brain size)*

```{r}
# Fit log(longevity)~log(brain size) model

longevity_log <- log(d$MaxLongevity_m)
brain_size_log <- log(d$Brain_Size_Species_Mean)

mod2 <- lm(longevity_log ~ brain_size_log, data = d)

# Extract coefficients from the model
intercept2 <- round(coef(mod2)[1], 2)
slope2 <- round(coef(mod2)[2], 2)

# Create the equation text
eq_text2 <- paste("Model 2: y =", slope2, "x", "+", intercept2)

plot2 <- ggplot(d, aes(brain_size_log, longevity_log, label = rownames(d)))+ geom_point() + geom_smooth(method = "lm", formula = y ~ x, color = "#32CBFF") + labs(title = "log(longevity)~log(brain size) model", x = "log(Brain Size (Species Mean))", y = "log(Max Longevity (Months))") + ##write titles
geom_text(aes(x = 2.5, y = 6.25, label = eq_text2), size = 4, color = "#32CBFF") ##write equation

plot2

```

#### [b] Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

##### *For longevity \~ brain size*

```{r}
summary_lm <- summary(mod1)
slope_estimate <- summary_lm["coefficients"] ##calculate slope
slope_estimate
```

β1 = 0.8789037

Since β1 does not equal zero, we reject the null hypothesis and accept the alternative hypothesis.

```{r}
confint(mod1, level = 0.9) ##calculate confidence interval
```

##### *For log(longevity) \~ log(brain size)*

```{r}
summary_log <- summary(mod2)
slope_estimate_log <- summary_log["coefficients"] ##calculate slope 
slope_estimate_log 
```

β1 = 0.2028543

Since β1 does not equal zero, we reject the null hypothesis and accept the alternative hypothesis.

```{r}
confint(mod2, level = 0.9) ##calculate confidence interval
```

#### [c] Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

##### *For longevity \~ brain size*

```{r}
ci <- predict(mod1, newdata = data.frame(brain_size), interval = "confidence", level = 0.90) ##calculate the confidence interval

info <- data.frame(cbind(brain_size, longevity, ci)) ##binding new data frame with confidence interval 

names(info) <- c("x", "y", "CIfit", "CIlwr", "CIupr") #naming the columns

pi <- predict(mod1, newdata = data.frame(brain_size), interval = "prediction", level = 0.90)  ##predict prediction interval 

info_ci_pi <- cbind(info, pi) ##binding updated data frame

names(info_ci_pi) <- c("x", "y", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr") ##updated names of columns

plot1b <- ggplot(data = info_ci_pi, aes(brain_size, longevity, label = rownames(d)))  ##drawing original plot with labels
plot1b <- plot1b + geom_point(alpha = 1/2) + geom_line(aes(x = x, y = CIfit, colour = "fit")) + geom_line(aes(x = x, y = CIlwr, colour = "CI")) + geom_line(aes(x = x, y = CIupr, colour = "CI")) + geom_line(aes(x = x, y = PIlwr, colour = "PI")) + geom_line(aes(x = x, y = PIupr, colour = "PI")) + scale_color_manual(values = c("fit" = "black","CI" = "#F96900","PI" = "#00A5E0")) + theme(legend.position = "bottom", legend.title = element_blank()) ##drawing points and lines

plot1b ##print graph 
```

##### *For log(longevity) \~ log(brain size)*

```{r}
ci2 <- predict(mod2, newdata = data.frame(Brain_Size_Log = brain_size_log), interval = "confidence", level = 0.90) ##calculate the confidence interval

info_log <- data.frame(cbind(brain_size_log, longevity_log, ci2)) ##binding new data frame with confidence interval 

names(info_log) <- c("x", "y", "CIfit", "CIlwr", "CIupr") #naming the columns

pi2 <- predict(mod2, newdata = data.frame(Brain_Size_Log = brain_size_log), interval = "prediction", level = 0.90)  ##predict prediction interval 

info_log_pi <- cbind(info_log, pi2) ##updating data frame

names(info_log_pi) <- c("x", "y", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr") #naming the columns

plot2b <- ggplot(data = info_log_pi, aes(brain_size_log, longevity_log, label = rownames(d))) 
plot2b <- plot2b + geom_point(alpha = 1/2) + geom_line(aes(x = x, y = CIfit, colour = "fit")) + geom_line(aes(x = x, y = CIlwr, colour = "CI")) + geom_line(aes(x = x, y = CIupr, colour = "CI")) + geom_line(aes(x = x, y = PIlwr, colour = "PI")) + geom_line(aes(x = x, y = PIupr, colour = "PI")) + scale_color_manual(values = c("fit" = "black","CI" = "#F96900","PI" = "#00A5E0")) + theme(legend.position = "bottom", legend.title = element_blank())

plot2b 

```

#### [d] Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm.

*For longevity \~ brain size*

```{r}
pi1 <- predict(mod1, newdata = data.frame(brain_size = 800), interval = "prediction", level = 0.90) ##predict method to calculate PI
pi1
```

*For log(longevity) \~ log(brain size)*

```{r}
pi2 <- predict(mod2, newdata = data.frame(brain_size_log = 800), interval = "prediction", level = 0.90) ##predict method to calculate PI
pi2
```

#### [e] Looking at your two models, which do you think is better? Why?

The logarithmic regression model is better because it normalizes skewed data. The confidence intervals are more centralized around the mean and the range of values is smaller.

## Challenges

1.  I struggled to generate the equation for the fit line in the original scatter plots. I was able to graph it but was confused about how to pull the data I needed for the equation. I hoped there was a method where I could just pull the line from the geom_smooth generated line but there wasn't. I then used the coefficients from the summary of the module to write the line.

2.  I also struggled to call the variables from the original data. I kept getting the atomic.vector warning and as a result couldn't use d\$variable to extract the element. I don't know why this was as I tested it in multiple locations and right after pulling the data, it claimed they were atomic. In office hours, we figured out we didn't even need to include the d\$ portion.

3.  I also struggled to find the prediction interval because of this calling issue. In office hours, we went over an easier method and how to format everything.

4.  I was unable to add the confidence and prediction interval bands to the original graphs. This is also tied to my variable issue. Whenever I try to call Brain_Size_Species_Mean it claims the variable doesn't exist or that the vector length is too long/doesn't match. I fixed this by storing the vectors into different smaller vectors at the start of the code. Jessica and Nicole helped me with this. They both had different suggestions for how to change everything so it ran smoothly. This made it so I didn't have to call it from original data and could keep everything organized.
